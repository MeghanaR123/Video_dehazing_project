{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeghanaR123/Video_dehazing_project/blob/main/flask_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmvptOsDBKi5",
        "outputId": "9cc4ea8a-78d0-4d65-bced-a7742550f5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4FpJz6L7oPo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms as tfs\n",
        "import glob\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "W7seehDGTZqX",
        "outputId": "058e14dc-c1e1-4ddd-85ff-34dd332c4517"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e853215364e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFFA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 )\n\u001b[1;32m   1493\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         return _legacy_load(\n\u001b[1;32m   1496\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "# Define the FFA model (Directly included here)\n",
        "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size // 2), bias=bias)\n",
        "\n",
        "class PALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(PALayer, self).__init__()\n",
        "        self.pa = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.pa(x)\n",
        "        return x * y\n",
        "\n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(CALayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.ca = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.ca(y)\n",
        "        return x * y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, conv, dim, kernel_size):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.act1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.calayer = CALayer(dim)\n",
        "        self.palayer = PALayer(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.act1(self.conv1(x))\n",
        "        res = res + x\n",
        "        res = self.conv2(res)\n",
        "        res = self.calayer(res)\n",
        "        res = self.palayer(res)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class Group(nn.Module):\n",
        "    def __init__(self, conv, dim, kernel_size, blocks):\n",
        "        super(Group, self).__init__()\n",
        "        modules = [Block(conv, dim, kernel_size) for _ in range(blocks)]\n",
        "        modules.append(conv(dim, dim, kernel_size))\n",
        "        self.gp = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.gp(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class FFA(nn.Module):\n",
        "    def __init__(self, gps, blocks, conv=default_conv):\n",
        "        super(FFA, self).__init__()\n",
        "        self.gps = gps\n",
        "        self.dim = 64\n",
        "        kernel_size = 3\n",
        "        pre_process = [conv(3, self.dim, kernel_size)]\n",
        "        assert self.gps == 3\n",
        "        self.g1 = Group(conv, self.dim, kernel_size, blocks=blocks)\n",
        "        self.g2 = Group(conv, self.dim, kernel_size, blocks=blocks)\n",
        "        self.g3 = Group(conv, self.dim, kernel_size, blocks=blocks)\n",
        "        self.ca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(self.dim * self.gps, self.dim // 16, 1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.dim // 16, self.dim * self.gps, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.palayer = PALayer(self.dim)\n",
        "        post_process = [conv(self.dim, self.dim, kernel_size), conv(self.dim, 3, kernel_size)]\n",
        "        self.pre = nn.Sequential(*pre_process)\n",
        "        self.post = nn.Sequential(*post_process)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x = self.pre(x1)\n",
        "        res1 = self.g1(x)\n",
        "        res2 = self.g2(res1)\n",
        "        res3 = self.g3(res2)\n",
        "        w = self.ca(torch.cat([res1, res2, res3], dim=1))\n",
        "        w = w.view(-1, self.gps, self.dim)[:, :, :, None, None]\n",
        "        out = w[:, 0, :] * res1 + w[:, 1, :] * res2 + w[:, 2, :] * res3\n",
        "        out = self.palayer(out)\n",
        "        x = self.post(out)\n",
        "        return x + x1\n",
        "\n",
        "# Configuration\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gps = 3\n",
        "blocks = 19\n",
        "\n",
        "# Load pre-trained model\n",
        "pretrained_model_dir = '/content/drive/MyDrive/synthetic-objective-testing-set-sots-reside/ffanet-pretrained-weights/' + f'ots_train_ffa_{gps}_{blocks}.pk'  # Update with your model path\n",
        "net = FFA(gps=gps, blocks=blocks)\n",
        "net = torch.nn.DataParallel(net)\n",
        "net.load_state_dict(torch.load(pretrained_model_dir, map_location=device)['model'])\n",
        "net.to(device)\n",
        "net.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9vg-qQ17mjx"
      },
      "outputs": [],
      "source": [
        "def reduce_video_resolution(input_path, output_path):\n",
        "    # Open the input video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # Get original width and height of the video\n",
        "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Calculate half resolution\n",
        "    new_width = original_width // 2\n",
        "    new_height = original_height // 2\n",
        "\n",
        "    # Get the frame rate of the original video\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Set up output video writer with half resolution and original fps\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print('break')\n",
        "            break\n",
        "\n",
        "        # Resize frame to half of original width and height\n",
        "        resized_frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Write the resized frame to the output video\n",
        "        out.write(resized_frame)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"Video saved with half resolution at:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp_Ia9x7F-Tg"
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path, output_folder, frame_limit=60):\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Load the video\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the total frame count\n",
        "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    total_frames = min(total_frames, frame_limit)  # Limit to the frame_limit\n",
        "\n",
        "    # Frame extraction loop\n",
        "    frame_number = 0\n",
        "    for i in tqdm(range(total_frames), desc=\"Extracting Frames\"):\n",
        "        success, frame = video_capture.read()\n",
        "        if not success or frame_number >= frame_limit:\n",
        "            break\n",
        "\n",
        "        # Save each frame as an image\n",
        "        frame_filename = os.path.join(output_folder, f\"frame_{frame_number:04d}.png\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "        frame_number += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    video_capture.release()\n",
        "    print(f\"Extraction complete. Frames saved to: {output_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "idVe5b9-TO5P",
        "outputId": "e9d503e0-186c-4971-ddf5-7c31983615f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-521d26c75d93>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFFA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 )\n\u001b[1;32m   1493\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         return _legacy_load(\n\u001b[1;32m   1496\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "# Set device to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Set model parameters\n",
        "gps = 3\n",
        "blocks = 19\n",
        "\n",
        "# Load pretrained model (update with your model path)\n",
        "pretrained_model_dir =  '/content/drive/MyDrive/synthetic-objective-testing-set-sots-reside/ffanet-pretrained-weights/' + f'ots_train_ffa_{gps}_{blocks}.pk'\n",
        "\n",
        "# Define FFA model (assuming the FFA class is already defined)\n",
        "net = FFA(gps=gps, blocks=blocks)\n",
        "net = nn.DataParallel(net)\n",
        "net.load_state_dict(torch.load(pretrained_model_dir, map_location=device)['model'])\n",
        "net.to(device)\n",
        "net.eval()\n",
        "\n",
        "# Folder paths\n",
        "input_frames_dir = '/content/drive/MyDrive/frames_folder/wood'  # Your input frames folder\n",
        "output_frames_dir = '/content/drive/MyDrive/dehazed_frames_folder/wood'  # Output folder for dehazed frames\n",
        "os.makedirs(output_frames_dir, exist_ok=True)\n",
        "\n",
        "# Set frame limit to 60\n",
        "frame_limit = 60\n",
        "\n",
        "# Process each frame up to the limit\n",
        "frame_count = 0\n",
        "for frame_file in tqdm(sorted(os.listdir(input_frames_dir)), desc=\"Dehazing Frames\"):\n",
        "    if frame_count >= frame_limit:\n",
        "        break\n",
        "\n",
        "    frame_path = os.path.join(input_frames_dir, frame_file)\n",
        "    hazy_image = Image.open(frame_path)\n",
        "\n",
        "    # Preprocess the frame\n",
        "    transform = tfs.Compose([\n",
        "        tfs.ToTensor(),\n",
        "        tfs.Normalize(mean=[0.64, 0.6, 0.58], std=[0.14, 0.15, 0.152])\n",
        "    ])\n",
        "    hazy_tensor = transform(hazy_image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Dehaze the frame\n",
        "    with torch.no_grad():\n",
        "        dehazed_tensor = net(hazy_tensor)\n",
        "\n",
        "    # Convert tensor to image and save\n",
        "    output_image = dehazed_tensor.squeeze(0).cpu().clamp(0, 1)\n",
        "    output_image = tfs.ToPILImage()(output_image)\n",
        "    output_image.save(os.path.join(output_frames_dir, frame_file))\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "print(f\"Dehazing complete. Frames saved to: {output_frames_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_uzm7yaMsgR"
      },
      "outputs": [],
      "source": [
        "def apply_ahe_to_folder(input_folder, output_folder, clip_limit=4.0, tile_grid_size=(8, 8)):\n",
        "    # Get a list of all images in the input folder\n",
        "    image_files = glob.glob(os.path.join(input_folder, \"*.*\"))\n",
        "\n",
        "    # Process each image with tqdm progress bar\n",
        "    for image_path in tqdm(image_files, desc=\"Processing images\"):\n",
        "        # Read the image\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Could not read image {image_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the image to LAB color space\n",
        "        lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "        # Split the LAB image to L, A, and B channels\n",
        "        l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
        "\n",
        "        # Apply CLAHE to the L channel\n",
        "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "        cl = clahe.apply(l_channel)\n",
        "\n",
        "        # Merge the CLAHE enhanced L channel with A and B channels\n",
        "        lab_img = cv2.merge((cl, a_channel, b_channel))\n",
        "\n",
        "        # Convert the LAB image back to BGR color space\n",
        "        enhanced_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        # Define the output path\n",
        "        output_path = os.path.join(output_folder, os.path.basename(image_path))\n",
        "\n",
        "        # Save the enhanced image to the output folder\n",
        "        cv2.imwrite(output_path, enhanced_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv6zm9kxELgd"
      },
      "outputs": [],
      "source": [
        "def create_video_from_frames(dehazed_frames_dir, output_video_path, frame_limit=60, frame_rate=30):\n",
        "\n",
        "    # Get the frames, limited by frame_limit\n",
        "    frame_files = sorted(os.listdir(dehazed_frames_dir))[:frame_limit]\n",
        "\n",
        "    # Ensure there are frames to process\n",
        "    if not frame_files:\n",
        "        print(\"No frames found in the specified directory.\")\n",
        "        return\n",
        "\n",
        "    # Read the first frame to get the frame size\n",
        "    first_frame_path = os.path.join(dehazed_frames_dir, frame_files[0])\n",
        "    frame = cv2.imread(first_frame_path)\n",
        "    height, width, layers = frame.shape\n",
        "\n",
        "    # Define video codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
        "\n",
        "    # Write frames to the video\n",
        "    for frame_file in tqdm(frame_files, desc=\"Creating Video\"):\n",
        "        frame_path = os.path.join(dehazed_frames_dir, frame_file)\n",
        "        frame = cv2.imread(frame_path)\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release the VideoWriter\n",
        "    video_writer.release()\n",
        "    print(f\"Video creation complete. Dehazed video saved to: {output_video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mhuVLwit-eu",
        "outputId": "b966c1ec-244a-4747-e4ec-73eb333bd585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Flask==3.0.0\n",
            "  Downloading flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting pyngrok==7.1.2\n",
            "  Downloading pyngrok-7.1.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from Flask==3.0.0) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask==3.0.0) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask==3.0.0) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask==3.0.0) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask==3.0.0) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok==7.1.2) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask==3.0.0) (3.0.2)\n",
            "Downloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/99.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.1.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok, Flask\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.1\n",
            "    Uninstalling Flask-3.1.1:\n",
            "      Successfully uninstalled Flask-3.1.1\n",
            "Successfully installed Flask-3.0.0 pyngrok-7.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask==3.0.0 pyngrok==7.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B13PQXvmuPD-"
      },
      "outputs": [],
      "source": [
        "# ngrok_key = \"2seRj7lTIywg503jYkqI9QIkQDs_4XjJECzPYZQw4hnRBrMGo\"\n",
        "ngrok_key = \"2qQQHtVmnB7cdMNktHEK0aFXuFd_aVWRYK9Qad895efrAN36\"\n",
        "port = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "17a6NJF4uTdy",
        "outputId": "d0178c5c-ba75-40c5-dfbc-7ffdc8fe52c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://7ece-34-125-102-191.ngrok-free.app'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(ngrok_key)\n",
        "ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxAYs-jFuWpu",
        "outputId": "57e93549-4725-410c-c9cf-c7a3dc599c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:43] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:45] \"GET /static/css/util.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:45] \"GET /static/vendor/animate/animate.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:45] \"GET /static/vendor/jquery/jquery-3.2.1.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:45] \"GET /static/vendor/select2/select2.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:45] \"GET /static/vendor/css-hamburgers/hamburgers.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:45] \"GET /static/css/main.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:45] \"GET /static/vendor/bootstrap/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:46] \"GET /static/fonts/font-awesome-4.7.0/css/font-awesome.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:46] \"GET /static/vendor/bootstrap/js/popper.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:46] \"GET /static/images/img-01.png HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:47] \"GET /static/vendor/select2/select2.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:47] \"GET /static/vendor/bootstrap/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:47] \"GET /static/vendor/tilt/tilt.jquery.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:47] \"GET /static/js/main.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:47] \"GET /static/fonts/font-awesome-4.7.0/fonts/fontawesome-webfont.woff2?v=4.7.0 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:48] \"GET /static/fonts/poppins/Poppins-Medium.ttf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:48] \"GET /static/fonts/poppins/Poppins-Bold.ttf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:48] \"GET /static/fonts/poppins/Poppins-Regular.ttf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:48] \"GET /static/fonts/montserrat/Montserrat-Bold.ttf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:55:50] \"GET /static/images/icons/favicon.ico HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:54] \"POST /login HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/vendor/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/vendor/animate/animate.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/css/main.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/fonts/font-awesome-4.7.0/css/font-awesome.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/css/util.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/vendor/jquery/jquery-3.2.1.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/vendor/css-hamburgers/hamburgers.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:55] \"\u001b[36mGET /static/vendor/select2/select2.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/images/img-01.png HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/js/main.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/vendor/select2/select2.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/vendor/tilt/tilt.jquery.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/fonts/poppins/Poppins-Bold.ttf HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/vendor/bootstrap/js/popper.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/fonts/poppins/Poppins-Medium.ttf HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/vendor/bootstrap/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/fonts/font-awesome-4.7.0/fonts/fontawesome-webfont.woff2?v=4.7.0 HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/fonts/montserrat/Montserrat-Bold.ttf HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:56:56] \"\u001b[36mGET /static/fonts/poppins/Poppins-Regular.ttf HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"POST /login HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/vendor/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/vendor/animate/animate.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/css/util.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/css/main.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/vendor/css-hamburgers/hamburgers.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/vendor/jquery/jquery-3.2.1.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/fonts/font-awesome-4.7.0/css/font-awesome.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:44] \"\u001b[36mGET /static/vendor/select2/select2.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/images/img-01.png HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/vendor/bootstrap/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/vendor/bootstrap/js/popper.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/js/main.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/fonts/poppins/Poppins-Medium.ttf HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/vendor/select2/select2.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/vendor/tilt/tilt.jquery.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:45] \"\u001b[36mGET /static/fonts/poppins/Poppins-Bold.ttf HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:46] \"\u001b[36mGET /static/fonts/poppins/Poppins-Regular.ttf HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:46] \"\u001b[36mGET /static/fonts/font-awesome-4.7.0/fonts/fontawesome-webfont.woff2?v=4.7.0 HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 05:57:46] \"\u001b[36mGET /static/fonts/montserrat/Montserrat-Bold.ttf HTTP/1.1\u001b[0m\" 304 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, render_template\n",
        "import os\n",
        "import shutil\n",
        "app = Flask(__name__, template_folder='/content/drive/MyDrive/code/templates',\n",
        "                      static_folder='/content/drive/MyDrive/code/static' )\n",
        "@app.route('/')\n",
        "def main():\n",
        "    return render_template(\"login.html\")\n",
        "\n",
        "@app.route('/login', methods = ['POST'])\n",
        "def login():\n",
        "  if request.method == 'POST':\n",
        "    un = request.form.get('email')\n",
        "    psd = request.form.get('pass')\n",
        "    if(un==\"admin@gmail.com\" and psd==\"admin\"):\n",
        "      return render_template(\"vindex.html\")\n",
        "    else:\n",
        "      return render_template(\"login.html\")\n",
        "  else:\n",
        "    return render_template(\"login.html\")\n",
        "\n",
        "@app.route('/success', methods = ['POST'])\n",
        "def success():\n",
        "    if request.method == 'POST':\n",
        "        f = request.files['file']\n",
        "        upload_folder = os.path.join(app.static_folder, '')\n",
        "        if not os.path.exists(upload_folder):\n",
        "            os.makedirs(upload_folder)\n",
        "        # Save the uploaded file\n",
        "        fname = os.path.join(upload_folder, f.filename)\n",
        "        f.save(upload_folder+'input_video.mp4')\n",
        "        print(upload_folder+'input_video.mp4')\n",
        "\n",
        "        # Path to the directory you want to manage\n",
        "        directory_path = '/content/drive/MyDrive/AHE/wood'\n",
        "\n",
        "        # Check if the directory exists\n",
        "        if os.path.exists(directory_path):\n",
        "            shutil.rmtree(directory_path)\n",
        "\n",
        "        # Create the new directory\n",
        "        os.makedirs(directory_path)\n",
        "\n",
        "        #===============write code=========#\n",
        "        output_video = '/content/drive/MyDrive/input_videos/wood.mp4'  # Path to save resized video\n",
        "        reduce_video_resolution(upload_folder+'input_video.mp4', output_video)\n",
        "\n",
        "        if os.path.exists('/content/drive/MyDrive/frames_folder/wood'): shutil.rmtree('/content/drive/MyDrive/frames_folder/wood')\n",
        "\n",
        "        #call extract_frames\n",
        "        extract_frames('/content/drive/MyDrive/input_videos/wood.mp4', '/content/drive/MyDrive/frames_folder/wood')\n",
        "\n",
        "        #call dehazing\n",
        "        # Set device to GPU if available\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        # Set model parameters\n",
        "        gps = 3\n",
        "        blocks = 19\n",
        "\n",
        "        # Load pretrained model (update with your model path)\n",
        "        pretrained_model_dir =  '/content/drive/MyDrive/synthetic-objective-testing-set-sots-reside/ffanet-pretrained-weights/' + f'ots_train_ffa_{gps}_{blocks}.pk'\n",
        "\n",
        "        # Define FFA model (assuming the FFA class is already defined)\n",
        "        net = FFA(gps=gps, blocks=blocks)\n",
        "        net = nn.DataParallel(net)\n",
        "        net.load_state_dict(torch.load(pretrained_model_dir, map_location=device)['model'])\n",
        "        net.to(device)\n",
        "        net.eval()\n",
        "\n",
        "        # Folder paths\n",
        "        input_frames_dir = '/content/drive/MyDrive/frames_folder/wood'  # Your input frames folder\n",
        "        output_frames_dir = '/content/drive/MyDrive/dehazed_frames_folder/wood'  # Output folder for dehazed frames\n",
        "        os.makedirs(output_frames_dir, exist_ok=True)\n",
        "\n",
        "        # Set frame limit to 60\n",
        "        frame_limit = 30\n",
        "\n",
        "        # Process each frame up to the limit\n",
        "        frame_count = 0\n",
        "        for frame_file in tqdm(sorted(os.listdir(input_frames_dir)), desc=\"Dehazing Frames\"):\n",
        "            if frame_count >= frame_limit:\n",
        "                break\n",
        "\n",
        "            frame_path = os.path.join(input_frames_dir, frame_file)\n",
        "            hazy_image = Image.open(frame_path)\n",
        "\n",
        "            # Preprocess the frame\n",
        "            transform = tfs.Compose([\n",
        "                tfs.ToTensor(),\n",
        "                tfs.Normalize(mean=[0.64, 0.6, 0.58], std=[0.14, 0.15, 0.152])\n",
        "            ])\n",
        "            hazy_tensor = transform(hazy_image).unsqueeze(0).to(device)\n",
        "\n",
        "            # Dehaze the frame\n",
        "            with torch.no_grad():\n",
        "                dehazed_tensor = net(hazy_tensor)\n",
        "\n",
        "            # Convert tensor to image and save\n",
        "            output_image = dehazed_tensor.squeeze(0).cpu().clamp(0, 1)\n",
        "            output_image = tfs.ToPILImage()(output_image)\n",
        "            output_image.save(os.path.join(output_frames_dir, frame_file))\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        print(f\"Dehazing complete. Frames saved to: {output_frames_dir}\")\n",
        "\n",
        "        #call AHE\n",
        "        # Set input and output folder paths\n",
        "        input_folder = \"/content/drive/MyDrive/dehazed_frames_folder/wood\"\n",
        "        output_folder = \"/content/drive/MyDrive/AHE/wood\"\n",
        "\n",
        "        # Apply AHE to all images in the input folder and save to output folder\n",
        "        apply_ahe_to_folder(input_folder, output_folder)\n",
        "\n",
        "        create_video_from_frames('/content/drive/MyDrive/AHE/wood', '/content/drive/MyDrive/code/static/wood_dehazed_video.mp4', frame_limit=200, frame_rate=30)\n",
        "\n",
        "        if os.path.exists('/content/drive/MyDrive/code/static/input_video_tmp.mp4'): os.remove('/content/drive/MyDrive/code/static/input_video_tmp.mp4')\n",
        "        if os.path.exists('/content/drive/MyDrive/code/static/wood_dehazed_video_tmp.mp4'): os.remove('/content/drive/MyDrive/code/static/wood_dehazed_video_tmp.mp4')\n",
        "\n",
        "        !ffmpeg -i /content/drive/MyDrive/code/static/input_video.mp4 -c:v libx264 -c:a aac -strict experimental /content/drive/MyDrive/code/static/input_video_tmp.mp4\n",
        "        !ffmpeg -i /content/drive/MyDrive/code/static/wood_dehazed_video.mp4 -c:v libx264 -c:a aac -strict experimental /content/drive/MyDrive/code/static/wood_dehazed_video_tmp.mp4\n",
        "\n",
        "        in_video = 'input_video_tmp.mp4'\n",
        "        return render_template(\"vindex.html\", input_video = in_video, output_video='wood_dehazed_video_tmp.mp4')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "17qtlDQr56RjgCAbAg3KnUfFt5qIDF28F",
      "authorship_tag": "ABX9TyP60Eg/mIi+xz/he/l5YJXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}